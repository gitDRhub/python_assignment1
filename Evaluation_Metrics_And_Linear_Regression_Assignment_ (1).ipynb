{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDPK1hWYN45I"
      },
      "outputs": [],
      "source": [
        "#Theoretical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 1 >>\n",
        "#R-squared (R²), also known as the coefficient of determination, is a key statistical measure in regression analysis that indicates how well the independent variables (predictors) in a model explain the variability of the dependent variable (response).\n",
        "#R² = 1: Perfect fit. This means the model explains all the variance in the dependent variable. In practice, this is rarely achieved because real-world data often have unexplained variability.\n",
        "#R² = 0: No explanatory power. This means the model does not explain any of the variance in the dependent variable, and the regression model is no better than simply using the mean of the dependent variable.\n",
        "#0 < R² < 1: Indicates a partial fit. The closer R² is to 1, the better the model explains the data. Conversely, the closer it is to 0, the less explanatory power the model has."
      ],
      "metadata": {
        "id": "HS78KokIOqST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 2 >>\n",
        "#Linear regression relies on several key assumptions to ensure the validity of the model and the reliability of its predictions. If these assumptions are violated, the results may be biased, inefficient, or misleading. Here are the main assumptions of linear regression:\n",
        "#1. Linearity:\n",
        "#The relationship between the dependent variable\n",
        "#This means that changes in the predictors are expected to cause proportional changes in the response variable.\n",
        "#Mathematically, this is expressed as:\n",
        "#If the relationship is non-linear, a linear regression model may not be appropriate, and non-linear regression or transformations might be needed.\n",
        "#2. Independence of Errors (No Autocorrelation):\n",
        "#The residuals (errors) should be independent of each other.\n",
        "#In time series data, for instance, there should be no correlation between consecutive error terms.\n",
        "#If errors are correlated (e.g., in a time series), the model might be overestimating or underestimating the significance of predictors, leading to incorrect conclusions.\n",
        "#Durbin-Watson test can be used to check for autocorrelation in residuals.\n",
        "#3. Homoscedasticity:\n",
        "#The variance of the residuals (errors) should be constant across all levels of the independent variables (i.e., the spread of the residuals should remain roughly the same across the range of fitted values).\n",
        "#If the residuals have varying spread (heteroscedasticity), the model's predictions and the standard errors might be biased, leading to unreliable significance tests.\n",
        "#This assumption can be checked with a residual vs. fitted value plot.\n",
        "#4. Normality of Errors:\n",
        "#The residuals should be approximately normally distributed. This assumption is crucial for hypothesis testing (such as t-tests for coefficients) and confidence intervals.\n",
        "#Normality ensures that the estimations of the regression coefficients and their significance levels are accurate.\n",
        "#This can be checked using Q-Q plots, histograms of residuals, or statistical tests like the Shapiro-Wilk test.\n",
        "#5. No Multicollinearity:\n",
        "#The independent variables should not be highly correlated with each other. If predictors are highly correlated (multicollinearity), it becomes difficult to determine the individual effect of each predictor on the dependent variable.\n",
        "#High multicollinearity can inflate standard errors and lead to unreliable coefficient estimates.\n",
        "#Multicollinearity can be assessed using Variance Inflation Factor (VIF) or correlation matrices.\n",
        "#6. Exogeneity (No Endogeneity):\n",
        "#The independent variables should be uncorrelated with the error term. This means that the predictors should not be influenced by factors that are not included in the model (i.e., no omitted variable bias).\n",
        "#If endogeneity exists (i.e., if an independent variable is correlated with the error term), the estimates of the coefficients will be biased and inconsistent.\n",
        "#This assumption is difficult to test directly, but careful model specification and variable selection can help avoid it.\n",
        "#7. No Measurement Error in Independent Variables:\n",
        "#The independent variables should be measured accurately. If there is measurement error in the predictors, it can lead to biased estimates of the regression coefficients.\n",
        "# Measurement error in the dependent variable does not bias the estimates of the\n",
        "# coefficients, but it can affect the precision."
      ],
      "metadata": {
        "id": "p7yMfm3HQazD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 3 >>\n",
        "#R-squared is useful for understanding the proportion of variance explained by the model, but it can be misleading when comparing models with different numbers of predictors.\n",
        "#Adjusted R-squared is a more reliable metric when comparing models with varying numbers of predictors because it penalizes the inclusion of unnecessary variables, helping to avoid overfitting."
      ],
      "metadata": {
        "id": "76L3ARqVRGm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 4 >>\n",
        "#Mean Squared Error (MSE) is a commonly used metric in regression and machine learning models to measure the quality of the model's predictions. MSE represents the average squared difference between the predicted values and the actual values in the data.\n",
        "\n",
        "#Why do we use Mean Squared Error (MSE)?\n",
        "\n",
        "#Penalty for Larger Errors:\n",
        "#By squaring the errors, MSE emphasizes larger mistakes. This is because errors are squared before being averaged. This means that larger discrepancies between predicted and actual values have a disproportionately large effect on the MSE.\n",
        "#This property helps to \"penalize\" models that make large errors, encouraging the model to minimize such errors during training.\n",
        "\n",
        "#Sensitivity to Outliers:\n",
        "#MSE is sensitive to outliers due to the squaring of the residuals. This is both a strength and a limitation.\n",
        "#Strength: It helps models learn to avoid large errors that would otherwise go unnoticed if the residuals were simply summed.\n",
        "#Limitation: If the data contains extreme outliers, the MSE can be heavily influenced by them, which might skew the model's performance if not properly handled.\n",
        "\n",
        "#Differentiability:\n",
        "#MSE is differentiable, meaning it has a well-defined derivative that can be used in optimization algorithms, such as gradient descent, which is commonly used in machine learning and regression. This property is crucial for model training because it allows for the efficient calculation of gradients during the fitting process.\n",
        "#The smoothness of MSE's curve makes it easier for optimization algorithms to converge to a minimum.\n",
        "\n",
        "#Model Evaluation:\n",
        "#MSE is widely used for model evaluation because it provides a clear, quantifiable metric for how far off the predictions are from the actual values. A lower MSE indicates that the model's predictions are closer to the true values, while a higher MSE indicates a poorer fit.\n",
        "#It provides an overall sense of the model's performance across all predictions, helping compare different models or variations of the same model.\n",
        "\n",
        "#Simple and Intuitive:\n",
        "#MSE is easy to understand and interpret. It represents the average squared difference between the predicted and actual values, making it a straightforward metric for assessing the accuracy of the model's predictions.\n",
        "\n",
        "#Comparative Measure:\n",
        "#MSE allows for easy comparison between different models. When training multiple models, comparing their MSE values can give insight into which model is performing better in terms of minimizing prediction error."
      ],
      "metadata": {
        "id": "Vrs7yb1zRHvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer 5 >>\n",
        "#An Adjusted R-squared value of 0.85 indicates that approximately\n",
        "# 85% of the variability in the dependent variable is explained by the\n",
        "# independent variables in the regression model, after adjusting for the number\n",
        "# of predictors (independent variables) used in the model. %%"
      ],
      "metadata": {
        "id": "h6Iqj5VESJwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 6 >>\n",
        "#Checking for normality of residuals is an important step in validating the assumptions of linear regression. The assumption of normality of residuals is necessary for performing valid hypothesis tests (such as t-tests for coefficients) and for constructing reliable confidence intervals"
      ],
      "metadata": {
        "id": "KZD_R8mOSTsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 7 >>\n",
        "#Multicollinearity refers to a situation in a regression model where two or more independent variables (predictors) are highly correlated with each other. In other words, one predictor can be linearly predicted from the others with a high degree of accuracy.\n",
        "#This creates problems because it makes it difficult to assess the individual effect of each predictor on the dependent variable.\n",
        "\n",
        "#Impact of Multicollinearity on Regression Models\n",
        "\n",
        "#Unstable Coefficient Estimates:\n",
        "#Multicollinearity leads to instability in the estimation of the regression coefficients. When the independent variables are highly correlated, it becomes difficult to determine which variable is actually influencing the dependent variable.\n",
        "#As a result, the coefficients of the correlated variables can become very large or very small, making them sensitive to small changes in the data. This means that small changes in the data could result in large fluctuations in the estimated coefficients, leading to unreliable results.\n",
        "\n",
        "#Increased Standard Errors:\n",
        "#When there is high multicollinearity, the standard errors of the coefficients increase. This means that the coefficients become less statistically significant, and it becomes harder to reject the null hypothesis (i.e., the coefficients are equal to zero).\n",
        "#Large standard errors increase the likelihood of Type II errors (failing to reject a false null hypothesis), leading to the incorrect conclusion that a predictor is not important when it actually is.\n",
        "\n",
        "#Overfitting:\n",
        "#Multicollinearity can contribute to overfitting of the regression model, especially in cases where you have many predictors. The model may appear to fit the data very well (low residuals), but it may not generalize well to new, unseen data. This is because the model may be too sensitive to small fluctuations in the data caused by the multicollinearity between predictors.\n",
        "\n",
        "#Difficulty in Interpreting the Model:\n",
        "#With multicollinearity, it is difficult to interpret the individual effect of each predictor on the dependent variable because the predictors are highly interrelated. The coefficients may not accurately reflect the independent contribution of each variable, as their effects are confounded with each other.\n",
        "#It becomes unclear whether the relationship between the predictors and the dependent variable is due to a specific predictor or due to the combined effect of multiple predictors.\n",
        "\n",
        "#Reduced Predictive Power:\n",
        "#While multicollinearity doesn’t necessarily reduce the overall predictive power of the model (in terms of R²), it can reduce the model’s ability to generalize to new data. This is because the model may rely on the correlated predictors in ways that don't hold for new data, thus leading to poorer performance on unseen data."
      ],
      "metadata": {
        "id": "4aASxxAXSpXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 8 >>\n",
        "#Mean Absolute Error (MAE) is a commonly used metric to measure the accuracy of a regression model by evaluating the average magnitude of the errors in a set of predictions.\n",
        "#The absolute error is the difference between the predicted value and the actual value, without considering whether the error is positive or negative.\n",
        "#The MAE provides a simple and intuitive measure of model performance."
      ],
      "metadata": {
        "id": "MYi37ATeTHsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 9 >>\n",
        "#An ML (Machine Learning) pipeline is a structured and automated sequence of steps that transforms raw data into a final model, optimizing the flow from data collection to model deployment.\n",
        "#Using an ML pipeline offers numerous benefits that enhance the efficiency, reproducibility, and scalability of machine learning processes. Below are the key benefits:\n",
        "\n",
        "#1. Automation and Efficiency:\n",
        "#Automates repetitive tasks: ML pipelines automate many stages of the machine learning process, such as data preprocessing, feature extraction, model training, and evaluation. This reduces the need for manual intervention and allows data scientists to focus on more complex tasks.\n",
        "#Faster model deployment: By automating routine tasks, ML pipelines can speed up the entire workflow, from data ingestion to model deployment, allowing teams to iterate and deploy models faster.\n",
        "\n",
        "#2. Reproducibility:\n",
        "#Ensures consistency: With an ML pipeline, all steps (such as data preprocessing, model training, and evaluation) are defined in a consistent and repeatable way. This ensures that the results of experiments can be reproduced, even if the model is retrained at a later time or under different conditions.\n",
        "#Version control: Pipelines can be versioned, meaning changes to data, code, or model configurations can be tracked. This improves the traceability of the entire machine learning process and supports reproducible research or deployments.\n",
        "\n",
        "#3. Scalability:\n",
        "#Handles larger datasets: As your data or model grows, an ML pipeline can scale accordingly. It can be optimized to handle large-scale data processing (e.g., using distributed computing frameworks like Apache Spark) and can handle more complex models.\n",
        "#Flexible for new data sources: An ML pipeline can be easily modified or extended to work with new data sources, allowing the model to adapt to evolving business needs or changes in the data.\n",
        "\n",
        "#4. Improved Collaboration:\n",
        "#Clear workflow for teams: ML pipelines provide a standardized workflow for teams, which is particularly useful in a collaborative environment. Different team members (e.g., data engineers, data scientists, software engineers) can work on specific stages of the pipeline in parallel.\n",
        "#Collaboration across platforms: Pipelines often support integration with different tools, cloud platforms, and version control systems, facilitating collaboration between various stakeholders.\n",
        "\n",
        "#5. Error Reduction:\n",
        "#Minimizes human errors: Since the steps are automated, human error is reduced. For instance, manual coding errors in data preprocessing or feature engineering can be minimized by using predefined pipeline steps.\n",
        "#Consistency in data processing: By using the same data processing and model training steps across different runs, the pipeline reduces the likelihood of introducing inconsistencies or errors in the process."
      ],
      "metadata": {
        "id": "jROKoctgTcCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 10 >>\n",
        "#Root Mean Squared Error (RMSE) is often considered more interpretable than Mean Squared Error (MSE) because of the following key reasons:\n",
        "\n",
        "#1. Same Units as the Original Data:\n",
        "#MSE is the average of the squared differences between the predicted and actual values. Since it squares the differences, the resulting value has units squared. For example, if you're predicting house prices (in dollars), MSE will be in square dollars.\n",
        "#RMSE, on the other hand, takes the square root of MSE, which brings the units back to the same units as the target variable (e.g., dollars). This makes RMSE directly interpretable in the context of the data you're working with.\n",
        "\n",
        "#2. More Direct Representation of Prediction Error:\n",
        "#RMSE gives a more intuitive sense of how much the model’s predictions deviate from the actual values, on average. It directly represents the average magnitude of error in the same units as the target variable, making it easier for stakeholders to understand the performance of the model.\n",
        "#In contrast, MSE is more abstract because of its squared units. Although MSE is useful for model optimization, it doesn't provide an immediate, straightforward sense of how much error the model is making on average.\n",
        "\n",
        "#3. Practical Use:\n",
        "#RMSE is often used in practice when you need to communicate model performance to non-technical stakeholders. For example, if you're working with a business team, telling them that the model's RMSE is 100 units (e.g., dollars, meters) is much easier for them to interpret compared to saying that the MSE is 10,000 units squared.\n",
        "#RMSE directly answers the question: \"On average, how far are the predictions from the actual values?\" This makes it highly useful in situations where the scale of the error is important for decision-making.\n",
        "\n",
        "#4. Smoothing Out Large Errors:\n",
        "#Both MSE and RMSE penalize large errors due to their squared nature, but RMSE's square root transformation helps moderate the impact of outliers, making the result a bit more interpretable than MSE. While this doesn't completely eliminate the sensitivity to large errors, the scale of RMSE helps provide a more balanced interpretation.\n",
        "#MSE can make large errors seem disproportionately significant due to the squaring of errors, but RMSE brings the error metric back to a more relatable scale."
      ],
      "metadata": {
        "id": "_OBqA4uoUAq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 11 >>\n",
        "#Pickling in Python refers to the process of serializing a Python object into a byte stream. This is done using the pickle module, which allows you to save Python objects (such as machine learning models, data structures, etc.) to a file and load them back later. Pickling is the reverse of unpickling, which is the process of converting the byte stream back into the original Python object.\n",
        "\n",
        "#Why Pickling is Useful in Machine Learning:\n",
        "\n",
        "#Saving and Loading Models:\n",
        "#In machine learning, once you’ve trained a model (such as a scikit-learn model, neural network, or any other model), you’ll likely want to save it for future use (e.g., to make predictions on new data without retraining). Pickling allows you to serialize the trained model and save it to a file, so you don't need to retrain it every time you want to use it.\n",
        "#You can then unpickle the saved model later to reuse it or deploy it into a production system, making your workflow much more efficient.\n",
        "\n",
        "#Efficient Model Storage:\n",
        "#Machine learning models can sometimes be large, and pickling provides an efficient way to save these large objects to a file. Once saved, the file can be stored or transferred as needed.\n",
        "#When you pickle a model, it’s saved in a binary format, which is generally more compact than other forms of saving models, such as plain text.\n",
        "\n",
        "#Model Persistence Across Sessions:\n",
        "#During model development, you may be working in a Jupyter notebook or other interactive environments where you train and evaluate models. Pickling allows you to persist models across different sessions or environments, meaning you don’t need to keep the entire training process running all the time or retrain your models after restarting your session.\n",
        "\n",
        "#Sharing and Deploying Models:\n",
        "#Sharing: After pickling a trained model, you can easily share it with colleagues or collaborators without needing to share the entire dataset or retrain the model. The pickled model can be transferred via files, emails, or cloud storage.\n",
        "#Deployment: Pickled models can be used in production systems. For example, once a model is trained and pickled, it can be loaded into a web service to make real-time predictions.\n",
        "\n",
        "#Saving Model States in Experimentation:\n",
        "#When experimenting with various algorithms or hyperparameters, pickling lets you save intermediate states of models or results. This way, you don’t have to start from scratch each time you want to try a new approach or tweak parameters."
      ],
      "metadata": {
        "id": "WXmpkxYQUW8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 12 >>\n",
        "#A high R-squared value (often denoted as R²) in a regression model indicates that the model explains a large proportion of the variance in the dependent variable (the variable you are trying to predict) based on the independent variables (the predictors)."
      ],
      "metadata": {
        "id": "C2FIcMDAUxOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 13 >>\n",
        "#When the assumptions of linear regression are violated, it can significantly affect the model's reliability, accuracy, and validity. Violating one or more assumptions can lead to misleading conclusions, inaccurate predictions, and poor generalization to new data. Below are the key linear regression assumptions, and what happens when they are violated:\n",
        "\n",
        "#1. Linearity (The relationship between the independent and dependent variables is linear)\n",
        "#Assumption: The relationship between the predictor(s) and the outcome variable is linear.\n",
        "#Violation: If the relationship is non-linear but you use a linear model, the model might underestimate or overestimate the effect of the predictors. This can lead to inaccurate predictions and biased coefficient estimates.\n",
        "#Solution: Apply transformations to the predictors (e.g., logarithmic, polynomial) or use non-linear models (e.g., decision trees, neural networks) to capture the non-linear relationships.\n",
        "\n",
        "#2. Independence of Errors (The residuals should be independent of each other)\n",
        "#Assumption: The residuals (errors) of the model should not be correlated with each other. This is particularly important in time-series data or data with an inherent order.\n",
        "#Violation: When this assumption is violated (i.e., there is autocorrelation), the errors are correlated, meaning that one observation's error depends on the next (e.g., in time-series data). This can lead to underestimated standard errors and inflated t-statistics, making the model seem more significant than it is.\n",
        "#Solution: Use models that account for autocorrelation, such as autoregressive (AR) models or generalized least squares (GLS). In time-series analysis, methods like differencing or lag variables may help address autocorrelation.\n",
        "\n",
        "#3. Homoscedasticity (Constant variance of errors)\n",
        "#Assumption: The variance of the residuals should be constant across all levels of the independent variable(s). This is called homoscedasticity.\n",
        "#Violation: If the variance of errors changes as the values of the independent variables change (known as heteroscedasticity), the model’s predictions become unreliable. This can lead to inefficient estimates and incorrect conclusions about the significance of predictors.\n",
        "#Solution: You can address heteroscedasticity by applying a log transformation to the dependent variable or by using weighted least squares (WLS) regression, which gives different weights to observations based on their variance.\n",
        "\n",
        "#4. Normality of Errors (The residuals should be normally distributed)\n",
        "#Assumption: The residuals should follow a normal distribution, especially for conducting hypothesis tests and constructing confidence intervals.\n",
        "#Violation: If the residuals are not normally distributed (e.g., skewed or have heavy tails), this can lead to inaccurate hypothesis testing and confidence intervals. However, normality is less important for prediction than for inference. The model still provides unbiased predictions, but statistical significance may be compromised.\n",
        "#Solution: For inference purposes, you can transform the dependent variable (e.g., using a log or square root transformation) to make the residuals more normal. Alternatively, robust regression methods can be used to mitigate the effects of non-normality.\n",
        "\n",
        "#5. No Multicollinearity (The independent variables should not be highly correlated)\n",
        "#Assumption: The independent variables should not be highly correlated with each other. High correlation between predictors is called multicollinearity.\n",
        "#Violation: When multicollinearity is present, it becomes difficult to determine the individual effect of each predictor on the dependent variable. This leads to unstable estimates of regression coefficients, large standard errors, and misleading significance tests.\n",
        "#Solution: To address multicollinearity, you can remove or combine highly correlated predictors, use Principal Component Analysis (PCA) to reduce dimensionality, or apply ridge regression or Lasso regression, which add a penalty term to shrink the coefficients."
      ],
      "metadata": {
        "id": "0uDoVhbtVQRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 14 >>\n",
        "#Multicollinearity can make it difficult to interpret the coefficients of regression models and lead to unstable estimates. To address multicollinearity, you can:\n",
        "\n",
        "#Remove highly correlated predictors.\n",
        "#Combine correlated variables (using PCA or domain knowledge).\n",
        "#Use regularization techniques such as Ridge regression or Lasso regression.\n",
        "#Check Variance Inflation Factors (VIF) to detect problematic predictors.\n",
        "#Increase the sample size when feasible.\n",
        "#Apply Principal Component Regression (PCR) or other dimensionality reduction techniques.\n",
        "\n",
        "#Each method has its pros and cons, and the appropriate solution depends on the nature of the data and the goals of the analysis. Regularization methods like Ridge and Lasso are particularly popular for handling multicollinearity without having to remove variables."
      ],
      "metadata": {
        "id": "fGLGsk5bVj-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 15 >>\n",
        "#In machine learning, pipelines are a crucial concept for organizing and automating the workflow, ensuring that the steps involved in building and deploying models are well-structured, repeatable, and efficient.\n",
        "# A machine learning pipeline consists of a series of steps that are applied to\n",
        "# the data sequentially, from preprocessing through to the final model\n",
        "# evaluation and deployment. %% Answer 16 >> The Adjusted R-squared is a\n",
        "# modified version of the R-squared that adjusts for the number of predictors\n",
        "# (independent variables) in a regression model. While R-squared increases as\n",
        "# more predictors are added, Adjusted R-squared accounts for the complexity of\n",
        "# the model and only increases when a new predictor improves the model more than\n",
        "# would be expected by chance. %% Answer 17 >>"
      ],
      "metadata": {
        "id": "McHKOXU-V5rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer 16 >>\n",
        "#The Adjusted R-squared is a modified version of the R-squared\n",
        "# that adjusts for the number of predictors (independent variables) in a\n",
        "# regression model. While R-squared increases as more predictors are added,\n",
        "# Adjusted R-squared accounts for the complexity of the model and only increases\n",
        "# when a new predictor improves the model more than would be expected by chance."
      ],
      "metadata": {
        "id": "1rDx1bwTWH1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 17 >>\n",
        "#Squaring the Residuals:\n",
        "\n",
        "#When calculating MSE, the residuals are squared.\n",
        "#Squaring amplifies large errors: If a data point has a large error (i.e., it is an outlier), its residual will be large, and when squared, it becomes even larger. For example, a residual of 10 becomes 100, and a residual of 100 becomes 10,000 when squared. This disproportionate increase in the error for outliers makes them have a significant effect on the overall MSE.\n",
        "#Outliers and Their Impact:\n",
        "\n",
        "#Outliers are extreme values that differ significantly from other data points in the dataset. These extreme errors cause large residuals, which when squared, disproportionately affect the MSE.\n",
        "#Since MSE takes the average of the squared residuals, a few outliers with large residuals can significantly increase the MSE, making it appear that the model is performing worse than it actually is for the majority of the data.\n",
        "#Lack of Robustness:\n",
        "\n",
        "#MSE is not robust to outliers because of the squaring operation. Even a small number of outliers can drastically increase the value of MSE, leading to misleading conclusions about the model's performance.\n",
        "#For example, in a dataset with mostly small errors (residuals close to 0), a few large residuals (from outliers) will have a disproportionately large effect on the final MSE."
      ],
      "metadata": {
        "id": "mwRzb3BfWYjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 18 >>\n",
        "#The Role of Homoscedasticity in Linear Regression:\n",
        "#Homoscedasticity plays a critical role in ensuring the validity and reliability of the results obtained from linear regression analysis. Here’s how it impacts the regression model:\n",
        "\n",
        "#1. Assumption for Valid Inferences:\n",
        "#One of the assumptions of linear regression is that the residuals exhibit homoscedasticity.\n",
        "#If the residuals are not homoscedastic (i.e., they show heteroscedasticity), the statistical tests and confidence intervals for the regression coefficients may become unreliable, which could lead to incorrect conclusions.\n",
        "\n",
        "#2. Efficient Estimation of Parameters:\n",
        "#Ordinary Least Squares (OLS), the most common method used to estimate the parameters (coefficients) in linear regression, assumes homoscedasticity. This assumption helps ensure that the OLS estimators are the Best Linear Unbiased Estimators (BLUE), according to the Gauss-Markov theorem.\n",
        "#Homoscedasticity ensures that the OLS estimators are efficient, meaning that they have the smallest possible variance among all unbiased estimators. If the residuals are heteroscedastic, OLS estimators are still unbiased, but they may not be efficient, and the standard errors of the coefficients may be distorted.\n",
        "\n",
        "#3. Impact on Statistical Significance:\n",
        "#If the residuals are homoscedastic, the regression analysis will provide accurate standard errors for the model coefficients.\n",
        "#If the residuals are heteroscedastic (i.e., the variance of the residuals changes across different levels of the independent variable(s)), the standard errors of the regression coefficients may be inflated or deflated, leading to incorrect significance tests. This could result in either Type I errors (incorrectly rejecting the null hypothesis) or Type II errors (failing to reject a false null hypothesis).\n",
        "\n",
        "#4. Diagnostic Checking:\n",
        "#Checking for homoscedasticity is a key part of regression diagnostics. If the residuals are not homoscedastic, it suggests that the model may be mis-specified or that there is a pattern in the data that has not been captured by the model.\n",
        "#Diagnostic plots, such as a residual vs. fitted values plot, are often used to visually inspect homoscedasticity. If the plot shows a pattern (for example, a funnel shape or increasing/decreasing spread of residuals), it indicates heteroscedasticity.\n",
        "\n",
        "#5. Influence on Model Predictions:\n",
        "#Homoscedasticity ensures that predictions made by the model are reliable across the range of the independent variable(s). In the case of heteroscedasticity, the model’s ability to predict outcomes with the same level of confidence for all data points may be compromised."
      ],
      "metadata": {
        "id": "iPGBkCeZW4og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 19 >>\n",
        "#Root Mean Squared Error (RMSE) is a commonly used metric to evaluate the accuracy of a regression model. It measures the average magnitude of the residuals, which are the differences between the observed and predicted values.\n",
        "#RMSE gives an indication of how well the model's predictions align with the actual data, with a lower RMSE indicating a better fit."
      ],
      "metadata": {
        "id": "gpmw8GjFXJ85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 20 >>\n",
        "#Pickling in Python provides a simple way to serialize and deserialize Python objects, but it carries significant security risks, particularly the potential for executing arbitrary code from untrusted sources. Because of these risks, it is generally recommended to avoid using pickle with data from unknown or untrusted origins.\n",
        "#For many applications, safer alternatives such as JSON or MessagePack should be preferred."
      ],
      "metadata": {
        "id": "HphlE3H4XfSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 21 >>\n",
        "#While pickle is often convenient, it carries security risks, especially when loading models from untrusted sources. There are numerous safer, more efficient alternatives for saving and deploying machine learning models, including:\n",
        "\n",
        "#Joblib (for large models and scikit-learn)\n",
        "#ONNX (for cross-framework compatibility)\n",
        "#TensorFlow SavedModel (for TensorFlow models)\n",
        "#HDF5 (especially for Keras and deep learning models)\n",
        "#PMML (for standardizing model exchange)\n",
        "#Core ML (for deploying on Apple devices)\n",
        "#LightGBM/XGBoost model formats (for gradient boosting models)\n",
        "#Choosing the right model-saving method depends on your use case, platform, and the type of model you are working with."
      ],
      "metadata": {
        "id": "OD3VxAYqXvsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 22 >>\n",
        "#In a simple linear regression, one of the assumptions is that the residuals (the differences between observed and predicted values) should have constant variance, also known as homoscedasticity. When this assumption is violated and the variance of the residuals changes systematically, it is called heteroscedasticity.\n",
        "\n",
        "#Homoscedasticity: The variance of residuals remains constant across all levels of the independent variable(s).\n",
        "#Heteroscedasticity: The variance of residuals changes across levels of the independent variable(s), often increasing or decreasing as the value of the predictor changes.\n",
        "\n",
        "#Why Is Heteroscedasticity a Problem?\n",
        "#Heteroscedasticity can be problematic for several reasons, particularly in the context of linear regression:\n",
        "\n",
        "#1. Inefficient Parameter Estimates:\n",
        "#One of the primary goals of linear regression is to estimate the parameters (coefficients) in a way that minimizes the sum of squared residuals.\n",
        "#When heteroscedasticity is present, the ordinary least squares (OLS) estimators can still be unbiased, but they are no longer efficient. This means that the estimates of the regression coefficients might be accurate on average, but they are less precise than they could be, and we lose the optimal properties of OLS.\n",
        "#The standard errors of the coefficients may be incorrect, which can lead to unreliable statistical tests (like t-tests and F-tests), potentially resulting in incorrect conclusions about the significance of variables.\n",
        "\n",
        "#2. Invalid Inference:\n",
        "#The presence of heteroscedasticity invalidates the statistical tests that rely on the assumption of constant variance of residuals. Specifically, confidence intervals and significance tests (such as t-tests and F-tests) for the regression coefficients may be misleading.\n",
        "#Incorrect standard errors lead to the wrong p-values, making it difficult to assess whether the model's coefficients are statistically significant.\n",
        "\n",
        "#3. Misleading Predictions:\n",
        "#If the residuals have unequal variance at different levels of the independent variables, predictions from the model will not be uniformly reliable across the range of predictor values.\n",
        "#In regions where the residuals exhibit larger variability, predictions will be less precise, leading to less reliable forecasts.\n",
        "\n",
        "#4. Violation of Model Assumptions:\n",
        "#Heteroscedasticity violates one of the key assumptions of linear regression, which assumes constant variance of errors (homoscedasticity). This violation can lead to problems in model diagnostics and result in inefficient estimations of regression parameters."
      ],
      "metadata": {
        "id": "n1Zlk8sDYFJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 23 >>\n",
        "#Adding irrelevant predictors (variables that do not have a meaningful relationship with the dependent variable) to a regression model can have distinct effects on R-squared and Adjusted R-squared. Let's explore these effects:\n",
        "\n",
        "\n",
        "#1. Effect on R-squared\n",
        "#R-squared (R²) measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. It ranges from 0 to 1, where a higher value indicates a better fit of the model to the data.\n",
        "\n",
        "#When you add irrelevant predictors (variables that do not truly affect the dependent variable), R-squared will always increase or remain the same. It cannot decrease. This is because adding more predictors, regardless of their relevance, will always explain more of the variance (even if that explanation is meaningless), thus artificially increasing R².\n",
        "\n",
        "#Why does this happen?\n",
        "\n",
        "#When you add a new predictor, the regression model has more flexibility to \"fit\" the data, and the sum of squared residuals will generally decrease (even if only slightly). This leads to an increase in the proportion of variance explained, thereby increasing R².\n",
        "#However, an increase in R² does not necessarily mean the model is improving in terms of predictive power or generalizability. Adding irrelevant predictors simply increases the model complexity, which may lead to overfitting.\n",
        "#2. Effect on Adjusted R-squared\n",
        "#Adjusted R-squared (Adjusted R²) is a modified version of R² that adjusts for the number of predictors in the model. It accounts for model complexity and penalizes the addition of irrelevant variables. Unlike R², Adjusted R² can decrease when irrelevant predictors are added to the model because it incorporates a penalty for the number of predictors, helping to prevent overfitting."
      ],
      "metadata": {
        "id": "tBSr3QdSYbm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "u6Y9LtgNYy3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 1 >>\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "print(diamonds.head())\n",
        "\n",
        "diamonds_clean = diamonds.dropna()\n",
        "\n",
        "X = diamonds_clean[['carat', 'depth', 'table']]\n",
        "y = diamonds_clean['price']\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Mean Absolute Error (MAE): {mae}')"
      ],
      "metadata": {
        "id": "FO8l54XbY1Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 2 >>\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "diamonds_clean = diamonds.dropna()\n",
        "\n",
        "X = diamonds_clean[['carat']]\n",
        "y = diamonds_clean['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "rmse = sqrt(mse)\n",
        "\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')"
      ],
      "metadata": {
        "id": "bzvd03LhY2YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 3 >>\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "diamonds_clean = diamonds.dropna()\n",
        "\n",
        "X = diamonds_clean[['carat', 'depth', 'table']]\n",
        "y = diamonds_clean['price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_const = sm.add_constant(X_train)\n",
        "\n",
        "model = sm.OLS(y_train, X_train_const).fit()\n",
        "\n",
        "y_pred = model.predict(sm.add_constant(X_test))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(diamonds_clean['carat'], diamonds_clean['price'], alpha=0.5)\n",
        "plt.xlabel('Carat')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Carat vs Price')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(diamonds_clean['depth'], diamonds_clean['price'], alpha=0.5)\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Depth vs Price')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(diamonds_clean['table'], diamonds_clean['price'], alpha=0.5)\n",
        "plt.xlabel('Table')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Table vs Price')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted Values (Homoscedasticity)')\n",
        "plt.show()\n",
        "\n",
        "correlation_matrix = X_train.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix of Predictors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W2sRVnwtZkrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 4 >\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = np.random.rand(100, 3)\n",
        "y = X @ np.array([1.5, -2.0, 3.0]) + 0.5\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "oPIY-Rz1bEoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 5 >>\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "data = fetch_california_housing()\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared score: {r2_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "id": "Z6EFMfqZ9_dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 6 >>\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(f\"Slope (Coefficient): {slope:.4f}\")\n",
        "print(f\"Intercept: {intercept:.4f}\")\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X, model.predict(X), color='red', label='Regression line')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.title('Linear Regression on Tips Dataset')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rwiDH9TH-D0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 7 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 2\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X, y_pred, color='red', label='Regression line')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Linear Regression - Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Slope (Coefficient): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
      ],
      "metadata": {
        "id": "3HGCD7dB-vGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 8 >>\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "with open('linear_regression_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "\n",
        "print(\"Model has been pickled and saved to 'linear_regression_model.pkl'\")"
      ],
      "metadata": {
        "id": "rP7oLEDT_AXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 9 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X**2 + 3 * X + 5 + np.random.randn(100, 1) * 10\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "y_range_pred = model.predict(X_range_poly)"
      ],
      "metadata": {
        "id": "rKB1k-5R_tJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 10 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "a = 3\n",
        "b = 5\n",
        "noise = np.random.randn(100, 1) * 2\n",
        "y = a * X + b + noise\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(f\"Coefficient (Slope): {model.coef_[0][0]:.4f}\")\n",
        "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
      ],
      "metadata": {
        "id": "RvlXZKU-_4yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 11 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X**3 - 3 * X**2 + 5 * X + 7 + np.random.randn(100, 1) * 50\n",
        "\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "y_range_pred = model.predict(X_range_poly)\n",
        "\n",
        "plt.plot(X_range, y_range_pred, color='red', label='Polynomial regression curve')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Polynomial Regression (Degree 3) on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jenU4Q0FAODv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 12 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X1 = np.random.rand(100, 1) * 10\n",
        "X2 = np.random.rand(100, 1) * 10\n",
        "\n",
        "noise = np.random.randn(100, 1) * 3\n",
        "y = 3 * X1 + 2 * X2 + 5 + noise\n",
        "\n",
        "X = np.hstack((X1, X2))\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(f\"Coefficients (for X1 and X2): {model.coef_[0]}\")\n",
        "print(f\"Intercept: {model.intercept_[0]}\")\n",
        "print(f\"R-squared score: {model.score(X, y):.4f}\")"
      ],
      "metadata": {
        "id": "1azZKjzkAZIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 13 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "noise = np.random.randn(100, 1) * 2\n",
        "y = 3 * X + 5 + noise\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "rmse = sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "8R-NAMR8A01R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 14 >>\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X1 = np.random.rand(100) * 10\n",
        "X2 = 2 * X1 + np.random.rand(100) * 5\n",
        "X3 = np.random.rand(100) * 10\n",
        "X4 = 0.5 * X3 + np.random.rand(100) * 5\n",
        "\n",
        "data = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4})\n",
        "\n",
        "X_with_const = add_constant(data)\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_with_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
        "\n",
        "print(vif_data)"
      ],
      "metadata": {
        "id": "PbV6Dat4BHFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 15 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "y = 2 * X**4 - 3 * X**3 + 4 * X**2 - 5 * X + 6 + np.random.randn(100, 1) * 100\n",
        "\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "y_range_pred = model.predict(X_range_poly)\n",
        "plt.plot(X_range, y_range_pred, color='red', label='Polynomial regression curve (degree 4)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Polynomial Regression (Degree 4) on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d7gcwn1nBTIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 16 >>\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 3) * 10\n",
        "\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] - 5 * X[:, 2] + 10 + np.random.randn(100) * 5\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "Pn7qQ_bNB4Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 17 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "y = 2 * X**3 - 3 * X**2 + 4 * X + 5 + np.random.randn(100, 1) * 100\n",
        "\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "X_range_poly = poly.transform(X_range)\n",
        "y_range_pred = model.predict(X_range_poly)\n",
        "\n",
        "plt.plot(X_range, y_range_pred, color='red', label='Polynomial regression curve (degree 3)')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Polynomial Regression (Degree 3) on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J_C9R2HCDqr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 18 >>\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 5) * 10\n",
        "\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] - 4 * X[:, 2] + 5 * X[:, 3] - 6 * X[:, 4] + 10 + np.random.randn(100) * 5\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "print(\"Model Coefficients: \", model.coef_)\n",
        "print(\"Model Intercept: \", model.intercept_)"
      ],
      "metadata": {
        "id": "_0DIzODJD9l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 19 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "y = 2 * X + 5 + np.random.randn(100, 1) * 2\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data points')\n",
        "plt.plot(X, y_pred, color='red', label='Regression line')\n",
        "plt.xlabel('Feature (X)')\n",
        "plt.ylabel('Target (y)')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Model Coefficients: {model.coef_}\")\n",
        "print(f\"Model Intercept: {model.intercept_}\")"
      ],
      "metadata": {
        "id": "dvUIWYXMEY3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 20 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 3) * 10\n",
        "\n",
        "y = 5 * X[:, 0] + 3 * X[:, 1] - 2 * X[:, 2] + 7 + np.random.randn(100) * 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "print(\"Model Coefficients: \", model.coef_)\n",
        "print(\"Model Intercept: \", model.intercept_)"
      ],
      "metadata": {
        "id": "vmuc9e5iEpUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 21 >>\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "y = 2 * X + 5 + np.random.randn(100, 1) * 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "with open('linear_regression_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n",
        "    print(\"Model saved to 'linear_regression_model.pkl'\")\n",
        "\n",
        "with open('linear_regression_model.pkl', 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)\n",
        "    print(\"Model loaded from 'linear_regression_model.pkl'\")\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "print(f\"Predictions: {y_pred[:5]}\")"
      ],
      "metadata": {
        "id": "SyQVX-QDFGPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 22 >>\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "tips_encoded = pd.get_dummies(tips, drop_first=True)\n",
        "\n",
        "X = tips_encoded.drop('tip', axis=1)\n",
        "y = tips_encoded['tip']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2:.4f}\")\n",
        "\n",
        "print(\"Model Coefficients: \", model.coef_)\n",
        "print(\"Model Intercept: \", model.intercept_)"
      ],
      "metadata": {
        "id": "62LuI6xdFRN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 23 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 3) * 10\n",
        "\n",
        "y = 5 * X[:, 0] + 3 * X[:, 1] - 2 * X[:, 2] + 7 + np.random.randn(100) * 2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "\n",
        "r2_linear = r2_score(y_test, y_pred_linear)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(f\"Linear Regression R-squared score: {r2_linear:.4f}\")\n",
        "print(\"Linear Regression Coefficients: \", linear_model.coef_)\n",
        "print(\"Linear Regression Intercept: \", linear_model.intercept_)\n",
        "\n",
        "print(f\"\\nRidge Regression R-squared score: {r2_ridge:.4f}\")\n",
        "print(\"Ridge Regression Coefficients: \", ridge_model.coef_)\n",
        "print(\"Ridge Regression Intercept: \", ridge_model.intercept_)"
      ],
      "metadata": {
        "id": "Nt8bIbX-GAWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 24 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validation R-squared scores: {cv_scores}\")\n",
        "print(f\"Average R-squared score: {cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "ZqnyqvpCGaIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 25 >>\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 1) * 10\n",
        "\n",
        "y = X**3 - 2*X**2 + 3 + np.random.randn(100, 1) * 5\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "degrees = [1, 2, 3, 4]\n",
        "r2_scores = []\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree)\n",
        "    X_poly_train = poly.fit_transform(X_train)\n",
        "    X_poly_test = poly.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_poly_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "    X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "    X_poly_range = poly.transform(X_range)\n",
        "    y_range_pred = model.predict(X_poly_range)\n",
        "    plt.plot(X_range, y_range_pred, label=f'Degree {degree} (R^2 = {r2:.4f})')\n",
        "\n",
        "plt.scatter(X, y, color='gray', alpha=0.5, label='Data points')\n",
        "\n",
        "plt.title('Polynomial Regression Models Comparison')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "for degree, score in zip(degrees, r2_scores):\n",
        "    print(f\"R-squared for Polynomial Degree {degree}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "Q14PL2wGGqP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer 26 >>\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.random.rand(100, 2) * 10\n",
        "\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] + 4 * X[:, 0] * X[:, 1] + np.random.randn(100) * 2\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
        "X_interaction = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_interaction, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Coefficients of the model with interaction terms:\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "\n",
        "feature_names = poly.get_feature_names_out(input_features=['X1', 'X2'])\n",
        "print(f\"Feature names (with interaction terms): {feature_names}\")"
      ],
      "metadata": {
        "id": "NsTkHsZNG3sa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}