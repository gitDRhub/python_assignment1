{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "**ans..** Logistic Regression is a classification algorithm used for binary/multiclass classification, while Linear Regression is for regression (continuous output)."
      ],
      "metadata": {
        "id": "c6K5L6HXNVPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "\n",
        "ans--P(Y=1)= 1/1+e‚àí(b0+b1X1+...+bnXn)\n"
      ],
      "metadata": {
        "id": "J83mu87ENeYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "ans-It converts any real number into a probability value between 0 and 1."
      ],
      "metadata": {
        "id": "QFaodfdZNiaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the cost function of Logistic Regression.\n",
        "\n",
        "ans--he log loss function:\n",
        "ùêΩ(ùúÉ)=‚àí1ùëö‚àë[ùë¶log‚Å°(‚Ñé)+(1‚àíùë¶)log\n",
        "(1‚àí‚Ñé)]J(Œ∏)=‚àím1‚àë[ylog(h)+(1‚àíy)log(1‚àíh)]"
      ],
      "metadata": {
        "id": "_R2Q2PWkNlfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "ans--Regularization (L1/L2) prevents overfitting by adding a penalty term to the cost function."
      ],
      "metadata": {
        "id": "t-kmHibQNpvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regressionC\n",
        "\n",
        "ans-Lasso (L1) shrinks coefficients and sets some to zero.\n",
        "Ridge (L2) shrinks coefficients but does not set them to zero.\n",
        "Elastic Net combines both L1 and L2."
      ],
      "metadata": {
        "id": "i0JVsfIzNsRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should we use Elastic Net instead of Lasso or Ridge.\n",
        "\n",
        "When features are highly correlated or when Lasso selects too few features."
      ],
      "metadata": {
        "id": "nUlV-JnSNufc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
        "\n",
        "ans--Higher Œª increases regularization, reducing overfitting but may cause underfitting."
      ],
      "metadata": {
        "id": "TCTN2uDeNxOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "ans--No multicollinearity, independent observations, and a linear relationship between log-odds and predictors."
      ],
      "metadata": {
        "id": "JJnS7x3kNzf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are some alternatives to Logistic Regression for classification tasks.\n",
        "\n",
        "ans--Decision Trees, SVM, Random Forest, Neural Networks, Na√Øve Bayes"
      ],
      "metadata": {
        "id": "zjquYicBN17g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics.\n",
        "\n",
        "ans--Accuracy, Precision, Recall, F1-score, ROC-AUC, Confusion Matrix."
      ],
      "metadata": {
        "id": "UvKJDFRHN4rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression.\n",
        "\n",
        "ans--It leads to biased predictions; solutions include class weighting, SMOTE, or threshold tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "u4gFPeQ8OPZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "\n",
        "ans--Adjusting parameters like C (regularization), penalty type, and solver for better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "_YU7Wf4TORrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "\n",
        "ans--‚Äòliblinear‚Äô (small datasets), ‚Äòsaga‚Äô (large datasets, L1), ‚Äòlbfgs‚Äô (multiclass)."
      ],
      "metadata": {
        "id": "BUcqMvNNOUUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How is Logistic Regression extended for multiclass classification.\n",
        "\n",
        "ans--Using One-vs-Rest (OvR) or Softmax (multinomial) regression."
      ],
      "metadata": {
        "id": "ZnIpa16AOWqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "\n",
        "ans--Pros: Simple, interpretable, efficient.\n",
        "Cons: Assumes linear decision boundary, sensitive to outliers."
      ],
      "metadata": {
        "id": "zq1InkNTOZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are some use cases of Logistic Regression.\n",
        "\n",
        "ans=-Spam detection, medical diagnosis, credit scoring, customer churn prediction."
      ],
      "metadata": {
        "id": "k09YZijYOb1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "\n",
        "ans--Softmax is for multiclass problems, Logistic is for binary classification.\n"
      ],
      "metadata": {
        "id": "69juQIKMOewA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "ans--OvR is simple and works well with few classes; Softmax is preferred for a large number of classes"
      ],
      "metadata": {
        "id": "OpcZ2AsmOhcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "ans--Each coefficient represents the change in log-odds for a unit increase in the predictor."
      ],
      "metadata": {
        "id": "x3HYAnwbOlwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**...practical part...**"
      ],
      "metadata": {
        "id": "rFFKY8w9Rl0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracyC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJG72lBPRpTl",
        "outputId": "e708fb54-31e5-4b9c-cf45-8b3571d3f147"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracyC\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M57svJYGS-vs",
        "outputId": "1929d3e6-d969-4662-93db-038f556f41c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Apply L2 Regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "print(\"Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UgDa9e4TMPR",
        "outputId": "ed4379e7-15f1-49a0-ad8d-af3254625c7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Coefficients: [[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Apply Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AiQKC4OTZNB",
        "outputId": "f486a7e1-8f0b-4bee-d816-fc886c0a06e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Write a Python program to train a Logistic Regression model for multiclass\n",
        "# classification using multi_class='ovr' C\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGcK337OT2jS",
        "outputId": "9aeff052-6306-47c7-ff8a-c85da2d2ecf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "#Regression. Print the best parameters and accuracyC\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vG4Cl0JUBfJ",
        "outputId": "ce7dad39-d465-4da0-d9bb-80d51cbfd33c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9583333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Write a Python program to evaluate Logistic Regression using Stratified\n",
        "# K-Fold Cross-Validation. Print the average accuracyC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=cv)\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSe4aFHiULvJ",
        "outputId": "16f1b146-9a34-46be-cc10-438f22bfd160"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "collapsed": true,
        "id": "lXmQ75JaUcjH",
        "outputId": "64092eb2-a9ff-4ea1-fc8e-4cf5544fb120"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3aa9e0764631>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "#Logistic Regression. Print the best parameters and accuracyM\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "random_search = RandomizedSearchCV(LogisticRegression(), param_dist, cv=5, n_iter=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgGD9YV-UzWh",
        "outputId": "33f4f1ab-c423-4aad-d39d-b014f61299d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic\n",
        "# Regression and print accuracyM\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "model = OneVsOneClassifier(LogisticRegression())\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34HE3gTkU4Iw",
        "outputId": "7bcac044-4aaf-4eca-e452-d016742eea2d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "#classification\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "loT-LovDVAhP",
        "outputId": "2eeca122-1262-4d85-9161-ec5e7db14f47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12.Write a Python program to train a Logistic Regression model and evaluate\n",
        "# its performance using Precision, Recall, and F1-ScoreM\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bh41hBQVhxX",
        "outputId": "22d1b4de-4891-47c3-e14d-33ff1300beb4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performanceM\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2nMy2mHVsIM",
        "outputId": "dca0a57d-0b92-46d9-ef47-c766e3f1d0ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performanceM\n",
        "df = pd.read_csv('titanic.csv')\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "X = df[['Pclass', 'Age', 'Fare']]  # Example features\n",
        "y = df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "SneI89d5WFTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "#model. Evaluate its accuracy and compare results with and without scalingM\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Accuracy:\", model.score(X_test_scaled, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA6cHgTEWtaP",
        "outputId": "64c6590d-8163-449d-beb4-7a2844b23f99"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16..Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "eD-PYpBiW4Sc",
        "outputId": "138aaa2a-0be0-4e70-f4f4-5570b8ebca64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "multi_class must be in ('ovo', 'ovr')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-45f3310eac0e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROC-AUC Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         return _multiclass_roc_auc_score(\n\u001b[1;32m    636\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17..Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracyM\n",
        "\n",
        "model = LogisticRegression(C=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIjSW9cqW_Zi",
        "outputId": "f08b04cd-1628-40c2-945e-c098a397595d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.. Write a Python program to train Logistic Regression and identify important features based on model coefficientsM\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "feature_importance = np.abs(model.coef_).flatten()\n",
        "print(\"Feature Importance:\", feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-7kxT0TXUxk",
        "outputId": "269a04bc-1a10-4f19-d588-cc1dbd703502"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance: [0.3792282  0.71689385 1.93173127 0.79860527 0.36046456 0.31444087\n",
            " 0.11412204 0.54499411 0.01876365 0.40245298 2.0458533  1.34359938]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa Score\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "print(\"Cohen‚Äôs Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jHl7nwWXcVp",
        "outputId": "c16b5627-7733-41c0-c559-f994939c2e48"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen‚Äôs Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Binarize the output for multi-class\n",
        "y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
        "# Assuming y_prob has probabilities for all classes, choose one for the curve\n",
        "y_prob_bin = y_prob[:, 1] # Example: probability for class 1\n",
        "\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test_bin[:,1], y_prob_bin) # probability for class 1\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "8C8Nc8ekXlr-",
        "outputId": "ae927f49-f226-41f8-a060-b58ef4cf09ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-520d43dab872>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_test_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Assuming y_prob has probabilities for all classes, choose one for the curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_prob_bin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Example: probability for class 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21..Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracyM\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Solver {solver}: Accuracy = {model.score(X_test, y_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfTflecdXrXI",
        "outputId": "408080ea-fa02-4aa9-8b28-6e7c44da1b93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver liblinear: Accuracy = 1.0\n",
            "Solver saga: Accuracy = 1.0\n",
            "Solver lbfgs: Accuracy = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)M\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "print(\"MCC Score:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "gg2fi1m-Xxd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23..Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scalingM\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "raw_acc = model.score(X_test, y_test)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "scaled_acc = model.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Raw Accuracy:\", raw_acc)\n",
        "print(\"Scaled Accuracy:\", scaled_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyIWsH79X2xz",
        "outputId": "bd44e922-4dc8-46f5-9d54-a4b0606527c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Accuracy: 1.0\n",
            "Scaled Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24.. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validationM\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "for C in [0.01, 0.1, 1, 10]:\n",
        "    model = LogisticRegression(C=C)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "    print(f\"C={C}: Accuracy = {scores.mean()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVvXYMCTX-Cf",
        "outputId": "476a0230-2d43-4bbe-84e4-2d0267910aec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.01: Accuracy = 0.8583333333333334\n",
            "C=0.1: Accuracy = 0.9333333333333333\n",
            "C=1: Accuracy = 0.9666666666666666\n",
            "C=10: Accuracy = 0.9416666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "\n",
        "print(\"Loaded Model Accuracy:\", loaded_model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "eq_7Zg6yYBZC",
        "outputId": "9403f9b6-ae87-4ca6-fd41-8aa728baefcc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-adafc428c31d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logistic_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded Model Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \"\"\"\n\u001b[1;32m    373\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexing_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mthis\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ]
    }
  ]
}